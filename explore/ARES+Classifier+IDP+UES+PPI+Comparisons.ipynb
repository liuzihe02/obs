{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc5iQhgvc1ms"
      },
      "source": [
        "# **ARES** Evaluation Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB2j0wpDL7dp"
      },
      "source": [
        "This notebook presents an in-depth exploration of the various configurations within **ARES**, highlighting comparative analyses and diverse evaluation strategies, including few-shot prompting and other frameworks such as RAGAS.\n",
        "\n",
        "**ARES** innovatively integrates synthetic data generation with fine-tuned classifiers to efficiently evaluate context relevance, answer faithfulness, and answer relevance, thereby reducing the reliance on extensive human annotations. By utilizing synthetic query generation and Prediction-Powered Inference (PPI), **ARES** ensures accurate evaluations with high statistical confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84b_0sTgHZNG"
      },
      "source": [
        "### 1) Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymP8Ne5yLo06"
      },
      "outputs": [],
      "source": [
        "# Optional for UES/IDP, configure API key for desired model(s)\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4K_6kpmPcDl"
      },
      "outputs": [],
      "source": [
        "# Download tutorial datasets\n",
        "\n",
        "!wget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_few_shot_prompt_for_judge_scoring.tsv\n",
        "!wget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_few_shot_prompt_for_synthetic_query_generation.tsv\n",
        "!wget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_labeled_output.tsv\n",
        "!wget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/example_files/nq_unlabeled_output.tsv\n",
        "!wget https://raw.githubusercontent.com/stanford-futuredata/ARES/main/datasets/eval_datasets/nq/nq_ratio_0.7.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cDNq5wyTC57"
      },
      "outputs": [],
      "source": [
        "# Download Synthetic Query Dataset\n",
        "\n",
        "# https://drive.google.com/file/d/1e5jXjScVIXb1lRD7YQ0ENPGteMibNDTO/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95nOtuqnPsH4"
      },
      "outputs": [],
      "source": [
        "# Download checkpoints for evaluation\n",
        "\n",
        "# Context Relevance: https://drive.google.com/file/d/1INyHfZpsUsn5UEBLSRehI9AX08AI12Lt/view?usp=sharing\n",
        "# Answer Relevance: https://drive.google.com/file/d/1yg1q6WrCwq7q07YceZUsd7FLVuLNJEue/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkRX9sh1TskE"
      },
      "outputs": [],
      "source": [
        "!export CUDA_VISIBLE_DEVICES = <specify GPUs>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzmFvn2PLz7f"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngNnQLsPc1mu"
      },
      "source": [
        "### 2) IDP + UES\n",
        "<p>Uses targeted prompts to enable pre-trained models to assess content relevance and accuracy in a zero-shot manner.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xyOohqec1mu"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "ues_idp_config = {\n",
        "    # Dataset for in-domain prompts\n",
        "    \"in_domain_prompts_dataset\": \"/content/nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
        "    # Dataset for unlabeled evaluation\n",
        "    \"unlabeled_evaluation_set\": \"/content/nq_unlabeled_output.tsv\",\n",
        "    # Model: GPT-3.5\n",
        "    \"model_choice\": \"gpt-3.5-turbo-0125\",\n",
        "}\n",
        "\n",
        "# Optional: Provide an alternative model of your choice below.\n",
        "# Here are some models you can choose from:\n",
        "# - mistralai/Mistral-7B-Instruct-v0.2\n",
        "# - mistralai/Mixtral-8x7B-Instruct-v0.1\n",
        "# - gpt-4-turbo-preview\n",
        "# - microsoft/deberta-v3-large\n",
        "# - openlm-research/open_llama_7b_v2\n",
        "# - mosaicml/mpt-7b-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FayDaXhc1mu"
      },
      "outputs": [],
      "source": [
        "ares = ARES(ues_idp=ues_idp_config)\n",
        "results = ares.ues_idp()\n",
        "print(results)\n",
        "\n",
        "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h9e-AQxc1mu"
      },
      "source": [
        "### 3) Training Classifier + IDP + UES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr4juU0Oc1mv"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "ues_idp_config = {\n",
        "    # Dataset for in-domain prompts\n",
        "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
        "    # Dataset for unlabeled evaluation\n",
        "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\",\n",
        "    # Model: GPT-3.5\n",
        "    \"model_choice\": \"gpt-3.5-turbo-0125\",\n",
        "}\n",
        "\n",
        "# Training Classifier\n",
        "classifier_config = {\n",
        "    \"training_dataset\": [\"nq_synth_queries.tsv\"],\n",
        "    \"validation_set\": [\"nq_ratio_0.7.tsv\"],\n",
        "    \"label_column\": [\"Context_Relevance_Label\"],\n",
        "    \"num_epochs\": 10,\n",
        "    \"patience_value\": 3,\n",
        "    \"learning_rate\": 5e-6,\n",
        "    \"assigned_batch_size\": 1,\n",
        "    \"gradient_accumulation_multiplier\": 32,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8tjD5iSc1mv"
      },
      "outputs": [],
      "source": [
        "ares_module = ARES(classifier_model=classifier_config)\n",
        "results = ares_module.train_classifier()\n",
        "print(results)\n",
        "\n",
        "# Trains and saves checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKJ6wsu_c1mv"
      },
      "outputs": [],
      "source": [
        "ares = ARES(ues_idp=ues_idp_config)\n",
        "results = ares.ues_idp()\n",
        "print(results)\n",
        "\n",
        "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U7FplrWc1mv"
      },
      "source": [
        "## 4) Training Classifier + PPI + UES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLlOK5Kc1mv"
      },
      "source": [
        "<h3>UES</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaczTIcrc1mv"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "ues_idp_config = {\n",
        "    # Dataset for in-domain prompts\n",
        "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
        "    # Dataset for unlabeled evaluation\n",
        "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\",\n",
        "    # Default model choice\n",
        "    \"model_choice\": \"gpt-3.5-turbo-1106\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjpM9CAhc1mv"
      },
      "outputs": [],
      "source": [
        "ares = ARES(ues_idp=ues_idp_config)\n",
        "results = ares.ues_idp()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbyCUvHYc1mw"
      },
      "source": [
        "<h3>Training Classifier</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HFZOEG7c1mw"
      },
      "source": [
        "<p>Generates checkpoint which is used in PPI below</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGJnHe51c1mw"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "classifier_config = {\n",
        "    \"training_dataset\": [\"nq_synth_queries.tsv\"],\n",
        "    \"validation_set\": [\"nq_ratio_0.7.tsv\"],\n",
        "    \"label_column\": [\"Context_Relevance_Label\"],\n",
        "    \"num_epochs\": 10,\n",
        "    \"patience_value\": 3,\n",
        "    \"learning_rate\": 5e-6,\n",
        "    \"assigned_batch_size\": 1,\n",
        "    \"gradient_accumulation_multiplier\": 32,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX5JRi2kc1mw"
      },
      "outputs": [],
      "source": [
        "ares = ARES(classifier_model=classifier_config)\n",
        "results = ares.train_classifier()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9-2CZFc1mw"
      },
      "source": [
        "<h3>PPI</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWA3pJ7yc1mw"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "ppi_config = {\n",
        "    \"evaluation_datasets\": [\"nq_unlabeled_output.tsv\"],\n",
        "    \"checkpoints\": [\"Context_Relevance_Label_joint_trained_date_time.pt\"],\n",
        "    \"labels\": [\"Context_Relevance_Label\"],\n",
        "    \"rag_type\": \"question_answering\",\n",
        "    \"gold_label_paths\": [\"nq_labeled_output.tsv\"],\n",
        "    \"prediction_filepaths\": [\"nq_0.6_predictions_updated.tsv\"],\n",
        "}\n",
        "\n",
        "# Install checkpoint here!\n",
        "# Context Relevance: https://drive.google.com/file/d/1INyHfZpsUsn5UEBLSRehI9AX08AI12Lt/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNVTrg7Fc1mw"
      },
      "outputs": [],
      "source": [
        "ares = ARES(ppi=ppi_config)\n",
        "results = ares.evaluate_RAG()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGEHOev9c1mw"
      },
      "source": [
        "## 5) ARES Comparison to RAGAS and Zeroshot Llama and Mixtral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m-vFqyhc1mw"
      },
      "source": [
        "<h3>ARES Configuration</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyWiTf0Ec1mw"
      },
      "source": [
        "<p>Synthetic Generator</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKwLLAcCc1mw"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "synth_config = {\n",
        "    \"document_filepaths\": [\"/content/nq_unlabeled_output.tsv\"],\n",
        "    \"few_shot_prompt_filenames\": [\"/content/nq_few_shot_prompt_for_judge_scoring.tsv\"],\n",
        "    \"synthetic_queries_filenames\": [\"nq_synthetic_queries.tsv\"],\n",
        "    \"documents_sampled\": 6189,\n",
        "}\n",
        "\n",
        "ares_module = ARES(synthetic_query_generator=synth_config)\n",
        "results = ares_module.generate_synthetic_data()\n",
        "print(results)\n",
        "\n",
        "# Generates and saves synthetic queries\n",
        "\n",
        "# Install Synthetic Query File here!\n",
        "# https://drive.google.com/file/d/1e5jXjScVIXb1lRD7YQ0ENPGteMibNDTO/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4pJSMYQc1mx"
      },
      "source": [
        "<p>Training Classifier</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4sSC3RSc1mx"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "classifier_config = {\n",
        "    \"training_dataset\": [\"nq_synth_queries.tsv\"],\n",
        "    \"validation_set\": [\"nq_ratio_0.7.tsv\"],\n",
        "    \"label_column\": [\"Context_Relevance_Label\", \"Answer_Relevance_Label\"],\n",
        "    \"num_epochs\": 10,\n",
        "    \"patience_value\": 3,\n",
        "    \"learning_rate\": 5e-6,\n",
        "    \"assigned_batch_size\": 1,\n",
        "    \"gradient_accumulation_multiplier\": 32,\n",
        "}\n",
        "\n",
        "ares = ARES(classifier_model=classifier_config)\n",
        "results = ares.train_classifier()\n",
        "print(results)\n",
        "\n",
        "# Trains and saves classifier for context relevance and answer relevance\n",
        "\n",
        "# Download checkpoints here!\n",
        "\n",
        "# Context Relevance: https://drive.google.com/file/d/1INyHfZpsUsn5UEBLSRehI9AX08AI12Lt/view?usp=sharing\n",
        "# Answer Relevance: https://drive.google.com/file/d/1yg1q6WrCwq7q07YceZUsd7FLVuLNJEue/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poWdw0kfc1mx"
      },
      "source": [
        "<p>PPI</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkYn3dONc1mx",
        "outputId": "2413b041-85a1-4b5a-8069-b73bd8ee7bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context_Relevance_Label Scoring\n",
            "ARES Ranking\n",
            "ARES Prediction: [0.6056978059262574]\n",
            "ARES Confidence Interval: [[0.547, 0.664]]\n",
            "Number of Examples in Evaluation Set: [4421]\n",
            "Ground Truth Performance: [0.6]\n",
            "ARES LLM Judge Accuracy on Ground Truth Labels: [0.789]\n",
            "Annotated Examples used for PPI: 300\n",
            "------------\n",
            "\n",
            "Answer_Relevance_Label Scoring\n",
            "ARES Ranking\n",
            "ARES Prediction: [0.5955191133227766]\n",
            "ARES Confidence Interval: [[0.577, 0.614]]\n",
            "Number of Examples in Evaluation Set: [4421]\n",
            "Ground Truth Performance: [0.6]\n",
            "ARES LLM Judge Accuracy on Ground Truth Labels: [0.977]\n",
            "Annotated Examples used for PPI: 300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ares import ARES\n",
        "\n",
        "ppi_config = {\n",
        "    \"evaluation_datasets\": [\"nq_unlabeled_output.tsv\"],\n",
        "    \"checkpoints\": [\n",
        "        \"Context_Relevance_Label_joint_trained_date_time.pt\",\n",
        "        \"Answer_Relevance_Label_joint_trained_date_time.pt\",\n",
        "    ],\n",
        "    \"rag_type\": \"question_answering\",\n",
        "    \"labels\": [\"Context_Relevance_Label\", \"Answer_Relevance_Label\"],\n",
        "    \"gold_label_path\": \"nq_labeled_output.tsv\",\n",
        "}\n",
        "\n",
        "ares_module = ARES(ppi=ppi_config)\n",
        "results = ares_module.evaluate_RAG()\n",
        "print(results)\n",
        "\n",
        "# Evaluation numbers below should match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0-LcdeQc1mx"
      },
      "source": [
        "<h3>RAGAS Configuration</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvvqMNDkc1mx"
      },
      "source": [
        "<p>Data Cleaning | Context Relevance Label Filter</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_UrINgrc1my"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(file_path):\n",
        "    # Load the dataset from the TSV file\n",
        "    dataset_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
        "\n",
        "    # Remove rows where 'Context_Relevance_Label' has no values\n",
        "    dataset_df = dataset_df.dropna(subset=[\"Context_Relevance_Label\"])\n",
        "\n",
        "    # Convert 'Context_Relevance_Label' to string if it is not already\n",
        "    dataset_df[\"Context_Relevance_Label\"] = dataset_df[\n",
        "        \"Context_Relevance_Label\"\n",
        "    ].astype(str)\n",
        "\n",
        "    # Use 'Context_Relevance_Label' as 'ground_truth'\n",
        "    prepared_data = {\n",
        "        \"question\": dataset_df[\"Query\"].tolist(),\n",
        "        \"contexts\": [\n",
        "            [doc] for doc in dataset_df[\"Document\"].tolist()\n",
        "        ],  # Contexts are expected to be list of lists\n",
        "        \"answer\": dataset_df[\"Answer\"].tolist(),\n",
        "        \"ground_truth\": dataset_df[\n",
        "            \"Context_Relevance_Label\"\n",
        "        ].tolist(),  # Using 'Context_Relevance_Label' as 'ground_truth'\n",
        "    }\n",
        "\n",
        "    # Convert to HuggingFace's Dataset format\n",
        "    dataset = Dataset.from_dict(prepared_data)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO6_1XBQc1my"
      },
      "source": [
        "<p> ARES Label Filter: Removes rows w/ no values for specified label</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxtzFOmdc1my"
      },
      "source": [
        "<p>Context Relevance Accuracy</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inWA-43Qc1my",
        "outputId": "d58db3f5-aacc-43da-86dc-c0adeaf1c62e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 8842/8842 [12:15<00:00, 12.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.5549, 'context_recall': 0.4737}\n"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import context_recall, context_precision\n",
        "\n",
        "# Load and prepare the dataset\n",
        "file_path = \"nq_unlabeled_output.tsv\"  # Update with the actual file path\n",
        "prepared_dataset = load_and_prepare_dataset(file_path)\n",
        "\n",
        "# Specify metrics\n",
        "metrics = [\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "]\n",
        "\n",
        "result = evaluate(prepared_dataset, metrics=metrics)  # Pass the initialized llm\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy3J9wDpc1my"
      },
      "source": [
        "<p>Data Cleaning | Answer Relevance Label Filter</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ohU4REhc1my"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(file_path):\n",
        "    # Load the dataset from the TSV file\n",
        "    dataset_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
        "\n",
        "    dataset_df = dataset_df.dropna(subset=[\"Answer_Relevance_Label\"])\n",
        "\n",
        "    # Convert 'Context_Relevance_Label' to string if it is not already\n",
        "    dataset_df[\"Answer_Relevance_Label\"] = dataset_df[\"Answer_Relevance_Label\"].astype(\n",
        "        str\n",
        "    )\n",
        "\n",
        "    # Use 'Context_Relevance_Label' as 'ground_truth'\n",
        "    prepared_data = {\n",
        "        \"question\": dataset_df[\"Query\"].tolist(),\n",
        "        \"contexts\": [[doc] for doc in dataset_df[\"Document\"].tolist()],\n",
        "        \"answer\": dataset_df[\"Answer\"].tolist(),\n",
        "        \"ground_truth\": dataset_df[\"Answer_Relevance_Label\"].tolist(),\n",
        "    }\n",
        "\n",
        "    # Convert to HuggingFace's Dataset format\n",
        "    dataset = Dataset.from_dict(prepared_data)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC2CUOatc1my",
        "outputId": "b23bd11d-63c4-4ebf-a025-2ff80fb39a06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:  92%|█████████▏| 4054/4421 [23:01<02:25,  2.52it/s]Failed to parse output. Returning None.\n",
            "Evaluating:  92%|█████████▏| 4063/4421 [23:04<02:01,  2.95it/s]Failed to parse output. Returning None.\n",
            "Evaluating: 100%|██████████| 4421/4421 [25:03<00:00,  2.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer_relevancy': 0.7511}\n"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import answer_relevancy\n",
        "\n",
        "file_path = \"nq_unlabeled_output.tsv\"\n",
        "prepared_dataset = load_and_prepare_dataset(file_path)\n",
        "\n",
        "# Specify metrics\n",
        "metrics = [answer_relevancy]\n",
        "\n",
        "# Evaluate\n",
        "result = evaluate(prepared_dataset, metrics=metrics)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OXxQ9TXc1my"
      },
      "source": [
        "<h3>Zeroshot Llama Configuration</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBDuO0n5c1mz"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "\n",
        "\n",
        "ues_idp_config = {\n",
        "    # Dataset for in-domain prompts\n",
        "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
        "    # Dataset for unlabeled evaluation\n",
        "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\",\n",
        "    # Model: Mistral 7B\n",
        "    \"model_choice\": \"codellama/CodeLlama-13b-Instruct-hf\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfWNktotc1mz"
      },
      "outputs": [],
      "source": [
        "ares = ARES(ues_idp=ues_idp_config)\n",
        "results = ares.ues_idp()\n",
        "print(results)\n",
        "\n",
        "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWYccnFdc1mz"
      },
      "source": [
        "<h3>Zeroshot Mistral Configuration</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "529wPxsCc1mz"
      },
      "outputs": [],
      "source": [
        "from ares import ARES\n",
        "import os\n",
        "\n",
        "ues_idp_config = {\n",
        "    # Dataset for in-domain prompts\n",
        "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
        "    # Dataset for unlabeled evaluation\n",
        "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\",\n",
        "    # Model: Mistral 7B\n",
        "    \"model_choice\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QA8sWllc1mz"
      },
      "outputs": [],
      "source": [
        "ares = ARES(ues_idp=ues_idp_config)\n",
        "results = ares.ues_idp()\n",
        "print(results)\n",
        "\n",
        "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
