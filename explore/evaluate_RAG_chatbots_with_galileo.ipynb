{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e8a996",
   "metadata": {},
   "source": [
    "# Evaluating 3 RAG Chatbots using ðŸ”­ Galileo Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16d63f",
   "metadata": {},
   "source": [
    "In this tutorial, we'll build 3 RAG based chatbots and evaluate the results in Galileo Evaluate.\n",
    "\n",
    "This notebook pulls data from the web for its datasource and uses Open AI for LLM. Feel Free to change these sources as you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd504ea-4d94-4868-ba40-a929e015de08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set-up of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f544bc-442f-4c3e-8fa2-c35907eb29bc",
   "metadata": {},
   "source": [
    "Let's start by installing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58c13e1-c5a0-47be-8cf0-c460fa788a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: promptquality in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (0.72.1)\n",
      "Requirement already satisfied: langchain in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-community in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain_openai in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (0.2.14)\n",
      "Requirement already satisfied: faiss-cpu in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: openai in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (1.58.1)\n",
      "Requirement already satisfied: ipywidgets in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (2.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (4.67.1)\n",
      "Requirement already satisfied: galileo-core<3.0.0,>=2.30.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (2.31.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.1.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (2.7.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (2.10.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.25.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from promptquality) (1.26.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: packaging in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: decorator in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: stack_data in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->promptquality) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->promptquality) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.1.0->promptquality) (1.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->promptquality) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->promptquality) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/flowingpurplecrane/obs-all/obs/venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install promptquality langchain langchain-community langchain_openai faiss-cpu openai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc56b04-eca5-48d5-93cc-f145bd02dedc",
   "metadata": {},
   "source": [
    "## 2. Set-up Galileo Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f2bc4-500c-4c71-9e52-f5eb8e0183da",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we will setup Galileo Evaluate client. Here we also define the metrics we wish to evaluate the models on. For this lab we will be using 9 metrics. Feel free to change them as needed and play aroundYou will need to enter 3 things - \n",
    " - GALILEO API KEY: This is the API key used to connect to the client. You can fetch this from the console\n",
    " - OPENAI API KEY: For this notebook we are using Open AI so enter your Open AI Key here. If you are using some other model, you can skip this\n",
    " - Project Name - Define a name for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb69761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to https://console.dev.rungalileo.io/get-token to generate a new Galileo token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: 882: x-www-browser: not found\n",
      "/usr/bin/xdg-open: 882: firefox: not found\n",
      "/usr/bin/xdg-open: 882: iceweasel: not found\n",
      "/usr/bin/xdg-open: 882: seamonkey: not found\n",
      "/usr/bin/xdg-open: 882: mozilla: not found\n",
      "/usr/bin/xdg-open: 882: epiphany: not found\n",
      "/usr/bin/xdg-open: 882: konqueror: not found\n",
      "/usr/bin/xdg-open: 882: chromium: not found\n",
      "/usr/bin/xdg-open: 882: chromium-browser: not found\n",
      "/usr/bin/xdg-open: 882: google-chrome: not found\n",
      "/usr/bin/xdg-open: 882: www-browser: not found\n",
      "/usr/bin/xdg-open: 882: links2: not found\n",
      "/usr/bin/xdg-open: 882: elinks: not found\n",
      "/usr/bin/xdg-open: 882: links: not found\n",
      "/usr/bin/xdg-open: 882: lynx: not found\n",
      "/usr/bin/xdg-open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'https://console.dev.rungalileo.io/get-token'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No token set. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGALILEO_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Enter Galileo key here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Enter Open AI Key here\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/obs-all/obs/venv/lib/python3.10/site-packages/promptquality/login_module.py:15\u001b[0m, in \u001b[0;36mlogin\u001b[0;34m(console_url)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mLogin to Galileo Environment.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mBy default, this will login to Galileo Cloud but can be used to login to the enterprise version of Galileo by\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mpassing in the console URL for the environment.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m set_config(console_url)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m~/obs-all/obs/venv/lib/python3.10/site-packages/promptquality/types/config.py:153\u001b[0m, in \u001b[0;36mConfig.login\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_login()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m SecretStr(token)\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241m.\u001b[39mget_current_user()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ‘‹ You have logged into ðŸ”­ Galileo (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsole_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_user\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/obs-all/obs/venv/lib/python3.10/site-packages/promptquality/types/config.py:129\u001b[0m, in \u001b[0;36mConfig.api_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refresh_token()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo token set. Please log in.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ApiClient(api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken)\n",
      "\u001b[0;31mValueError\u001b[0m: No token set. Please log in."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import promptquality as pq\n",
    "\n",
    "os.environ[\"GALILEO_CONSOLE_URL\"] = \"https://console.dev.rungalileo.io\"\n",
    "os.environ[\"GALILEO_API_KEY\"] = \"\"  # Enter Galileo key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Enter Open AI Key here\n",
    "pq.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f4b885-0fd7-49d9-8676-424a0603a2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from promptquality import EvaluateRun\n",
    "\n",
    "PROJECT_NAME = \"evaluate-rag-chatbot\"\n",
    "metrics = [\n",
    "    pq.Scorers.context_adherence_luna,\n",
    "    pq.Scorers.completeness_luna,\n",
    "    pq.Scorers.correctness,\n",
    "    pq.Scorers.chunk_attribution_utilization_luna,\n",
    "    pq.Scorers.instruction_adherence_plus,\n",
    "    pq.Scorers.sexist,\n",
    "    pq.Scorers.tone,\n",
    "    pq.Scorers.toxicity,\n",
    "    pq.Scorers.prompt_injection,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcfaa9-16f9-49c5-9b2b-60221bbf9bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Loading and Preparing Data\n",
    "\n",
    "For this lab we will use a fictuous use case where we want to build a chatbot to answer questions about Galileo. A typical technique to build such a chatbot is Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "Now in order to build the chatbot, we will \n",
    " - Fetch some documents from Galileo's website, \n",
    " - Create some questions, \n",
    " - Ask the chatbot those questions\n",
    " - Evaluate the chatbot's responses with the help of Galileo Evaluate\n",
    "\n",
    "In our case let's start by downloading some documents for Galileo from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba938331-e2ed-4eb1-a907-9828d320807b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load data from a website URL\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://docs.rungalileo.io/galileo\",\n",
    "    \"https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-guardrail-metrics/context-adherence\",\n",
    "    \"https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-guardrail-metrics/context-relevance\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f2255-2a7c-4cb5-b8c7-1a50b58d9027",
   "metadata": {},
   "source": [
    "Now that the context data in the form of the documents has been downloaded we will now split them into smaller text chunks using the Langchain library. The CharacterTextSplitter divides the text into chunks of a specified size while allowing for overlap to prevent cutting sentences in half. When setting the chunk size, make sure it fits into the context window of your LLM and feel free to experiment with different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791829cf-a1c6-449d-b98f-e9c66c9a331f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a548e60-0e64-4b9d-bc11-31ec9b491643",
   "metadata": {},
   "source": [
    "Let's have a look at the size of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8b103-c80c-4e84-8c33-d2a3502f6227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print metadata of the loaded documents\n",
    "avg_doc_length = lambda documents: sum(\n",
    "    [len(doc.page_content) for doc in documents]\n",
    ") // len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(texts)\n",
    "print(\n",
    "    f\"Average length among {len(documents)} pages loaded is {avg_char_count_pre} characters.\"\n",
    ")\n",
    "print(f\"After the split you have {len(texts)}\")\n",
    "print(f\"Average length among {len(texts)} chunks is {avg_char_count_post} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6457d-0f85-4229-9e35-fd4510b385a0",
   "metadata": {},
   "source": [
    "Next we convert our chunks into embeddings and store them in a vector database. This is a common technique used in RAG where instead of always passing all the documents to the LLM as context, we will pull the chunks we feel are most relevant to a given question and only pass those to the LLM. This is achieved by doing a semantic similarity search within the vector DB between the question embeddings and the chunk embeddings. Passing concise information to the LLM helps improve its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc90761-6b98-47b8-8c86-ab45d7a3c211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66065ed2-1b4b-42dc-830b-d4dcb10c2dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run Inference for 3 models with Galileo Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a099df-8c78-4878-8c0c-089ea66e088f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function to generate a response using Open AI\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def generate_response(prompt: str, history: list = [], model_name: str = \"gpt-4o-mini\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "    )\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "\n",
    "    return response_text, input_tokens, output_tokens, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92673e-9d92-40d2-aa96-ae594919ed53",
   "metadata": {},
   "source": [
    "If you want to type in your own questions on the fly for the LLM for a richer chatbot experience, set `USE_PREDEFINED_QUESTIONS` to False. Otherwise the model will run on these pre-defined questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4fcfcb-7238-480b-a87e-0a7251c4ec63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_PREDEFINED_QUESTIONS = True\n",
    "\n",
    "questions = [\n",
    "    \"What does Galileo do?\",\n",
    "    \"What are some of the RAG Metrics Galileo provides?\",\n",
    "    \"What is LUNA and where is it used?\",\n",
    "    \"How does LUNA calculate context adherence?\",\n",
    "    \"What is chainpoll?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60468525",
   "metadata": {},
   "source": [
    "Here we define the models we want to evaluate, and the system prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c9687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "models = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"o1-mini\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd136dc9-75fb-4b0c-ad6e-dd6eaec6f9c7",
   "metadata": {},
   "source": [
    "Now let's run the actual inference and log the information to Galileo! If you want to run the LLM chat longer, set the `max_rounds` variable accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rounds = 5\n",
    "\n",
    "for model_name in models[1:]:\n",
    "    rounds = 0\n",
    "    evaluate_run = EvaluateRun(\n",
    "        run_name=model_name, project_name=PROJECT_NAME, scorers=metrics\n",
    "    )\n",
    "    history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    while rounds < max_rounds:\n",
    "        question = questions[rounds] if USE_PREDEFINED_QUESTIONS else input(\"You: \")\n",
    "        if question.lower() == \"exit\":\n",
    "            break\n",
    "        # Retrieve relevant documents from the vector store\n",
    "        relevant_docs = vectorstore.similarity_search(question, k=3)\n",
    "        context_list = [doc.page_content for doc in relevant_docs]\n",
    "        context = \" \".join(context_list)\n",
    "        prompt = f\"\"\"Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer: \"\"\"\n",
    "\n",
    "        # Create your workflow to log to Galileo.\n",
    "        wf = evaluate_run.add_workflow(\n",
    "            input={\"question\": question, \"model_name\": model_name},\n",
    "            name=model_name,\n",
    "            metadata={\"env\": \"demo\"},\n",
    "        )\n",
    "        wf.add_retriever(\n",
    "            input=question,\n",
    "            documents=context_list,\n",
    "            metadata={\"env\": \"demo\"},\n",
    "            name=f\"{model_name}_RAG\",\n",
    "        )\n",
    "\n",
    "        # Generate the response with the updated history\n",
    "        model_response, input_tokens, output_tokens, total_tokens = generate_response(\n",
    "            prompt, history, model_name\n",
    "        )\n",
    "\n",
    "        # Add the current question to the history\n",
    "        history.append({\"role\": \"user\", \"content\": question})\n",
    "        # Update history with the new interaction\n",
    "        history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "        print(\"You: \", question)\n",
    "        print(f\"Assistant: {model_response}\")\n",
    "        print(\"*\" * 100)\n",
    "\n",
    "        # Log your llm call step to Galileo.\n",
    "        wf.add_llm(\n",
    "            input=prompt,\n",
    "            output=model_response,\n",
    "            model=model_name,\n",
    "            input_tokens=input_tokens,\n",
    "            output_tokens=output_tokens,\n",
    "            total_tokens=total_tokens,\n",
    "            metadata={\"env\": \"demo\"},\n",
    "            name=f\"{model_name}_QA\",\n",
    "        )\n",
    "\n",
    "        # Conclude the workflow.\n",
    "        wf.conclude(output={\"output\": model_response})\n",
    "        rounds += 1\n",
    "    evaluate_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcb67f",
   "metadata": {},
   "source": [
    "You can have a look at the final results in the console via the link generated from the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d5206-8856-4719-aec2-59d2f2b07e97",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Throughout this notebook, we have explored the process of creating and evaluation a chatbot for a QA-RAG application using GPT 4o mini via Open AI, Python, and Langchain. We covered essential steps, including setting up the environment, loading and preparing context data, extracting relevant context, answer generation, and logging to Galileo."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
